use aion_parse::rebase::get_total_rebase_daily_episode;
use aion_types::rebase::rebase_daily::RebaseDaliy;
use all_in_one_news::configuration::get_configuration;
use all_in_one_news::startup::get_connection_pool;
use all_in_one_news::startup::Application;
use anyhow::Result;
use sqlx::PgPool;
use std::sync::Arc;
use std::sync::Mutex;
use std::time::Duration;
use tokio::{task, time};
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};
use uuid::Uuid;

async fn task_handler(rebase_daily: RebaseDaliy, conn_pool: Arc<Mutex<PgPool>>) {
    let mut connection_pool = conn_pool
        .lock()
        .unwrap()
        .acquire()
        .await
        .expect("Failed to acquire connection");
    println!("å®šæ—¶ä»»åŠ¡æ‰§è¡Œä¸­...");
    let key_id = Uuid::new_v4();
    // åœ¨è¿™é‡Œç¼–å†™ä½ çš„å®šæ—¶ä»»åŠ¡é€»è¾‘
    // æ‰§è¡Œæ’å…¥æ“ä½œ
    // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒ ID çš„è®°å½•
    let existing_record = sqlx::query!(
        "SELECT id FROM rebase_daily WHERE id = $1",
        rebase_daily.id as i32
    )
    .fetch_optional(connection_pool.as_mut())
    .await;

    match existing_record {
        Ok(Some(_)) => {
            println!("ç›¸åŒ ID çš„è®°å½•å·²å­˜åœ¨ï¼Œä¸æ‰§è¡Œæ’å…¥æ“ä½œ");
        }
        Ok(None) => {
            // å¦‚æœä¸å­˜åœ¨ç›¸åŒ ID çš„è®°å½•ï¼Œåˆ™æ‰§è¡Œæ’å…¥æ“ä½œ
            let tags = rebase_daily
                .tag
                .iter()
                .map(|v| v.to_string())
                .collect::<Vec<String>>();
            let result = sqlx::query!(
                   r#"
                   INSERT INTO rebase_daily (key_id, id, author, episode, introduce, time, title, url, tag)
                   VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                   "#,
                   key_id,
                   rebase_daily.id as i32,
                   rebase_daily.author,
                   rebase_daily.episode,
                   rebase_daily.introduce,
                   rebase_daily.time,
                   rebase_daily.title,
                   rebase_daily.url,
                   &tags // æ³¨æ„æ­¤å¤„ä½¿ç”¨å¼•ç”¨æ¥æ’å…¥ Vec<String>
               )
               .execute(connection_pool.as_mut())
               .await;

            match result {
                Ok(_) => println!("æ’å…¥æˆåŠŸ"),
                Err(e) => println!("æ’å…¥å¤±è´¥: {:?}", e),
            }
        }
        Err(e) => {
            println!("æ£€æŸ¥è®°å½•æ—¶å‡ºé”™: {:?}", e);
        }
    }
}

pub async fn process_load_all_rebase_daily(conn_pool: Arc<Mutex<PgPool>>) -> anyhow::Result<()> {
    let total_rebase_daily_episode = get_total_rebase_daily_episode().await?;
    for item in total_rebase_daily_episode {
        let conn_pool = conn_pool.clone();
        dbg!(&item);
        task_handler(RebaseDaliy::try_from(item)?, conn_pool).await;
    }
    Ok(())
}

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::registry()
        .with(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| "aion=trace,tower_http=debug".into()),
        )
        .with(tracing_subscriber::fmt::layer())
        .init();

    let configuration = get_configuration()?;

    let pg_pool = get_connection_pool(&configuration.database);
    let pg = Arc::new(Mutex::new(pg_pool));

    // let task = task::spawn(async move {
    //     loop {
    //         let pg = pg.clone();
    //         // task_handler(pg).await;
    //         time::sleep(Duration::from_secs(60 * 60 * 24)).await; // æ¯éš”24å°æ—¶æ‰§è¡Œä¸€æ¬¡å®šæ—¶ä»»åŠ¡
    //     }
    // });
    //
    process_load_all_rebase_daily(pg).await?;

    // let service = Application::build(configuration).await?;
    // println!("ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ Server is running on port 8000 ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ");

    // service.run_until_stopped().await?;

    // task.await?;

    Ok(())
}
